{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# [3/9] LSTM & GRU\n",
    "# =============================================\n",
    "# 목표: 장기 의존성 문제를 해결한 LSTM, GRU의 성능을 RNN과 비교합니다.\n",
    "\n",
    "# --- 1. 기본 설정 (이전 단계와 유사) ---\n",
    "!pip install torch torchtext transformers datasets scikit-learn matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# --- 2. 데이터 준비 (이전 단계와 동일) ---\n",
    "# (02_rnn_classification.ipynb의 데이터 준비 코드를 여기에 그대로 붙여넣으세요.)\n",
    "# ... (생략) ...\n",
    "# 데이터 로더까지 준비되었다고 가정합니다.\n",
    "\n",
    "# --- 3. LSTM / GRU 모델 정의 ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # nn.LSTM 사용\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # LSTM은 hidden state와 cell state를 튜플로 반환\n",
    "        # output: (batch, seq_len, hidden_dim), (hidden, cell): ((n_layers, batch, hidden), (n_layers, batch, hidden))\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # 마지막 레이어의 hidden state 사용\n",
    "        last_hidden = hidden[-1, :, :]\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # nn.GRU 사용\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # GRU는 hidden state만 반환\n",
    "        output, hidden = self.gru(embedded)\n",
    "        last_hidden = hidden[-1, :, :]\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# --- 4. 모델 학습 및 평가 함수 ---\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_acc += (preds == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    accuracy = total_acc / total_count\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Training time: {end_time - start_time:.2f}s\")\n",
    "    return accuracy\n",
    "\n",
    "# --- 5. 성능 비교 실험 ---\n",
    "# 하이퍼파라미터 (은닉 크기, 레이어 수 튜닝 가능)\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2 # 레이어 수를 2로 늘려 실험\n",
    "\n",
    "# RNN 모델 (비교를 위해 다시 정의)\n",
    "# ... (02_rnn_classification.ipynb의 RNNClassifier 클래스 코드) ...\n",
    "# rnn_model = RNNClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "# print(\"--- RNN Performance ---\")\n",
    "# train_and_evaluate(rnn_model, train_loader, test_loader)\n",
    "\n",
    "# LSTM 모델\n",
    "lstm_model = LSTMClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS)\n",
    "print(\"\\n--- LSTM Performance ---\")\n",
    "train_and_evaluate(lstm_model, train_loader, test_loader)\n",
    "\n",
    "# GRU 모델\n",
    "gru_model = GRUClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS)\n",
    "print(\"\\n--- GRU Performance ---\")\n",
    "train_and_evaluate(gru_model, train_loader, test_loader)\n",
    "\n",
    "print(\"\\n결론: 일반적으로 LSTM과 GRU가 RNN보다 더 높은 성능을 보이며, 장기 의존성 포착에 유리합니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
