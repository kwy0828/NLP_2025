{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# [7/9] 프리트레인 모델①: BERT\n",
    "# =============================================\n",
    "# 목표: BERT 모델을 Hugging Face 라이브러리로 불러와 감성 분류 태스크에 파인튜닝합니다.\n",
    "\n",
    "# --- 1. 필수 패키지 설치 ---\n",
    "!pip install transformers datasets scikit-learn torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# --- 2. 데이터셋 및 모델/토크나이저 로드 ---\n",
    "# 이번에는 한국어 데이터셋인 NSMC(Naver Sentiment Movie Corpus)를 사용해봅니다.\n",
    "dataset = load_dataset(\"nsmc\", cache_dir=\"./.cache\")\n",
    "model_name = \"klue/bert-base\"  # 한국어 BERT 모델\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# --- 3. 데이터 전처리 ---\n",
    "def tokenize_function(examples):\n",
    "    # max_length, batch_size 등 다양한 설정 비교 가능\n",
    "    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 실습을 위해 데이터셋 일부만 사용\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# --- 4. Trainer API로 학습 스크립트 작성 ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # 간단한 실습을 위해 1 에폭만\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 5. 학습 및 평가 ---\n",
    "print(\"BERT 파인튜닝을 시작합니다...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n학습 완료! 평가를 시작합니다...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"평가 결과: {eval_results}\")\n",
    "\n",
    "# --- 6. 간단한 추론 ---\n",
    "text = \"이 영화 정말 재미있어요! 배우들 연기가 최고네요.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(f\"\\n입력: '{text}'\")\n",
    "print(f\"예측: {'긍정(1)' if predicted_class_id == 1 else '부정(0)'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
