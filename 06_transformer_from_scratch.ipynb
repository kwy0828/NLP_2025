{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# [6/9] 셀프 어텐션(Self-Attention) & Transformer\n",
    "# =============================================\n",
    "# 목표: 트랜스포머의 핵심인 멀티헤드 셀프어텐션의 구조를 직접 구현하며 이해합니다.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# --- 1. Scaled Dot-Product Attention 구현 ---\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch_size, n_heads, seq_len, d_k)\n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # 마스킹: 어텐션 스코어 행렬에서 마스크 값이 0인 위치를 아주 작은 값(-1e9)으로 치환\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context, attention\n",
    "\n",
    "# --- 2. Multi-Head Attention 구현 ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Apply attention on all heads\n",
    "        context, attn = self.attention(q_s, k_s, v_s, mask)\n",
    "        \n",
    "        # 3. Concat and additional linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        output = self.W_O(context)\n",
    "        return output\n",
    "\n",
    "# --- 3. 작은 Transformer 모델 빌드 (개념적 구조) ---\n",
    "# 실제 모델은 Positional Encoding, FeedForward Network, LayerNorm 등이 추가됩니다.\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # Multi-Head Attention -> Add & Norm\n",
    "        _src = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.norm1(src + self.dropout(_src))\n",
    "        # FeedForward -> Add & Norm\n",
    "        _src = self.ffn(src)\n",
    "        src = self.norm2(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "print(\"ScaledDotProductAttention, MultiHeadAttention, EncoderLayer가 정의되었습니다.\")\n",
    "print(\"\\n[실습 과제]\")\n",
    "print(\"1. PositionalEncoding 클래스를 구현해보세요. (sin, cos 함수 사용)\")\n",
    "print(\"2. DecoderLayer를 구현해보세요. (Masked Self-Attention + Encoder-Decoder Attention)\")\n",
    "print(\"3. 이 구성 요소들을 조립해 전체 Transformer 모델을 완성하고, Seq2Seq 과제에 적용해보세요.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
