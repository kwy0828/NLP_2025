{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# [8/9] 프리트레인 모델②: GPT 계열 & 생성 태스크\n",
    "# =============================================\n",
    "# 목표: GPT 모델의 텍스트 생성 방식을 이해하고, 다양한 샘플링 기법을 실습합니다.\n",
    "\n",
    "!pip install transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. 모델 및 토크나이저 로드 ---\n",
    "# 한국어 GPT 모델 (KoGPT2)\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# --- 2. 간단한 텍스트 생성 ---\n",
    "prompt = \"인공지능 모델은 인간에게\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"--- 프롬프트: {prompt} ---\")\n",
    "# 기본(Greedy Search) 생성\n",
    "sample_outputs = model.generate(input_ids, max_length=50)\n",
    "print(\"\\n[기본 생성 결과]\")\n",
    "print(tokenizer.decode(sample_outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# --- 3. generate() 파라미터 변화 실험 ---\n",
    "def generate_text(prompt, temp, top_k, top_p):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True, # 샘플링을 하려면 True로 설정\n",
    "        temperature=temp,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    print(f\"\\n--- temp={temp}, top_k={top_k}, top_p={top_p} ---\")\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "prompt_for_sampling = \"오늘 날씨가 좋아서,\"\n",
    "\n",
    "# Temperature: 높을수록 창의적(랜덤), 낮을수록 결정적\n",
    "generate_text(prompt_for_sampling, temp=0.7, top_k=50, top_p=1.0) # 약간 창의적\n",
    "generate_text(prompt_for_sampling, temp=1.2, top_k=50, top_p=1.0) # 매우 창의적\n",
    "\n",
    "# Top-k sampling: 확률이 높은 k개 중에서만 샘플링\n",
    "generate_text(prompt_for_sampling, temp=1.0, top_k=10, top_p=1.0) # 상위 10개 단어만 사용\n",
    "\n",
    "# Top-p (Nucleus) sampling: 확률 합이 p를 넘는 최소 단어 집합에서 샘플링\n",
    "generate_text(prompt_for_sampling, temp=1.0, top_k=0, top_p=0.92) # 확률 합 92% 내 단어만 사용\n",
    "\n",
    "# --- 4. 전이 태스크 적용 (Prompting) ---\n",
    "print(\"\\n--- 프롬프트를 이용한 작업 수행 ---\")\n",
    "\n",
    "# 요약\n",
    "summary_prompt = \"\"\"\n",
    "텍스트: 자연어 처리(NLP)는 인공지능의 한 분야로, 컴퓨터가 인간의 언어를 이해하고, 해석하며, 생성할 수 있도록 하는 기술이다. 최근 트랜스포머 아키텍처의 등장으로 NLP 기술은 비약적인 발전을 이루었으며, 기계 번역, 텍스트 요약, 챗봇 등 다양한 분야에 응용되고 있다.\n",
    "요약:\n",
    "\"\"\"\n",
    "input_ids = tokenizer.encode(summary_prompt, return_tensors='pt').to(device)\n",
    "summary_output = model.generate(input_ids, max_length=150, repetition_penalty=1.2)\n",
    "print(\"\\n[요약 예시]\")\n",
    "print(tokenizer.decode(summary_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
