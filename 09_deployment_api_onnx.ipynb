{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# [9/9] 종합 실습 & 배포\n",
    "# =============================================\n",
    "# 목표: 학습된 모델을 ONNX로 변환해 보고, FastAPI를 이용한 API 서버 코드를 작성합니다.\n",
    "# (이 노트북은 Colab에서 직접 실행하는 부분과, 로컬에서 실행할 코드 예시로 구성됩니다)\n",
    "\n",
    "# --- 1. ONNX 변환 및 추론 (Colab에서 실행 가능) ---\n",
    "!pip install transformers torch onnx onnxruntime\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Step 7에서 파인튜닝한 모델이 있다고 가정. 여기서는 원본 모델로 대체.\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# 더미 입력 생성\n",
    "dummy_text = \"이것은 ONNX 변환을 위한 샘플 문장입니다.\"\n",
    "dummy_input = tokenizer(dummy_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "onnx_model_path = \"bert_classifier.onnx\"\n",
    "\n",
    "# torch.onnx.export로 모델 변환\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "    onnx_model_path,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'logits': {0: 'batch_size'}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(f\"모델이 {onnx_model_path}에 저장되었습니다.\")\n",
    "\n",
    "# ONNX Runtime으로 추론\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# PyTorch 추론 결과와 비교\n",
    "with torch.no_grad():\n",
    "    torch_logits = model(**dummy_input).logits.numpy()\n",
    "\n",
    "# ONNX 추론\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: dummy_input['input_ids'].numpy(),\n",
    "              ort_session.get_inputs()[1].name: dummy_input['attention_mask'].numpy()}\n",
    "ort_logits = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "# 결과 비교\n",
    "np.testing.assert_allclose(torch_logits, ort_logits, rtol=1e-03, atol=1e-05)\n",
    "print(\"\\nONNX 추론 결과가 PyTorch 결과와 일치합니다!\")\n",
    "\n",
    "# --- 2. FastAPI 서버 구현 (로컬 실행용 코드) ---\n",
    "print(\"\\n--- 아래는 로컬 환경에서 실행할 FastAPI 서버 코드 예시입니다. ---\")\n",
    "\n",
    "fastapi_code = \"\"\"\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# FastAPI 앱 초기화\n",
    "app = FastAPI()\n",
    "\n",
    "# 모델 및 토크나이저 로드 (앱 시작 시 한 번만)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "ort_session = ort.InferenceSession(\"bert_classifier.onnx\")\n",
    "\n",
    "# 요청 Body 모델 정의\n",
    "class Item(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(item: Item):\n",
    "    # 1. 입력 텍스트 토크나이징\n",
    "    inputs = tokenizer(\n",
    "        item.text, return_tensors=\"np\", \n",
    "        padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "    ort_inputs = {\n",
    "        'input_ids': inputs['input_ids'].astype(np.int64),\n",
    "        'attention_mask': inputs['attention_mask'].astype(np.int64)\n",
    "    }\n",
    "\n",
    "    # 2. ONNX 런타임으로 추론\n",
    "    ort_logits = ort_session.run(None, ort_inputs)[0]\n",
    "    \n",
    "    # 3. 결과 처리\n",
    "    prediction = int(np.argmax(ort_logits, axis=1)[0])\n",
    "    probability = float(np.exp(ort_logits).sum(axis=1)[0])\n",
    "    \n",
    "    return {\"prediction\": prediction, \"text\": item.text}\n",
    "\n",
    "# 실행 방법 (터미널):\n",
    "# 1. pip install fastapi uvicorn python-multipart\n",
    "# 2. 위 코드를 main.py로 저장\n",
    "# 3. uvicorn main:app --reload\n",
    "\"\"\"\n",
    "print(fastapi_code)\n",
    "\n",
    "\n",
    "# --- 3. Docker 이미지화 (로컬 실행용 파일 예시) ---\n",
    "print(\"\\n--- 아래는 FastAPI 앱을 Docker로 패키징하기 위한 파일 예시입니다. ---\")\n",
    "\n",
    "dockerfile_content = \"\"\"\n",
    "# 1. 베이스 이미지 선택\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# 2. 작업 디렉토리 설정\n",
    "WORKDIR /app\n",
    "\n",
    "# 3. 의존성 파일 복사 및 설치\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# 4. 앱 소스코드와 모델 파일 복사\n",
    "COPY ./main.py .\n",
    "COPY ./bert_classifier.onnx .\n",
    "\n",
    "# 5. 포트 노출\n",
    "EXPOSE 8000\n",
    "\n",
    "# 6. 앱 실행 명령어\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "requirements_content = \"\"\"\n",
    "fastapi\n",
    "uvicorn\n",
    "onnxruntime\n",
    "transformers\n",
    "torch\n",
    "numpy\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[Dockerfile 내용]\")\n",
    "print(dockerfile_content)\n",
    "print(\"\\n[requirements.txt 내용]\")\n",
    "print(requirements_content)\n",
    "print(\"\\n실행 명령어 (터미널):\")\n",
    "print(\"docker build -t nlp-api .\")\n",
    "print(\"docker run -d -p 8000:8000 nlp-api\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
